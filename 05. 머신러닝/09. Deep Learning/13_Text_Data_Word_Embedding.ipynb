{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59629849",
   "metadata": {},
   "source": [
    "### 2021_11_12_2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf20f9cc",
   "metadata": {},
   "source": [
    "## 단어 임베딩(word embedding) 이해하기\n",
    "### 학습 내용\n",
    "* 단어 임베딩 기본 개념 이해\n",
    "* IMDB 영화 리뷰 데이터 셋을 활용하여 단어 임베딩 실습해 보기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe3ca3b",
   "metadata": {},
   "source": [
    "### 단어 임베딩 이해하기\n",
    "* 단어와 벡터를 연관짓는 강력하고 인기 있는 또 다른 방법 중의 하나는 단어 임베딩이다.\n",
    "* 단어 임베딩은 밀집 단어 벡터를 사용하는 것.\n",
    "* 단어 임베딩은 언어를 기하학적 공간에 매핑하는 것.\n",
    "* 잘 구축된 임베딩 공간에서는 동의어가 비슷한 단어 벡터로 임베딩된다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a4fb483",
   "metadata": {},
   "source": [
    "### Onehot vs 단어 임베딩 비교\n",
    "* A. 고차원 저차원\n",
    "    * 원핫 인코딩 기법으로 만든 벡터는 대부분 0으로 채워지는 고차원이다.\n",
    "    * 단어 임베딩은 저차원 기법이다.\n",
    "    * 원핫인코딩은 단어 사전이 2만개의 토큰으로 이루어져 있다면 20,000차원의 벡터를 사용\n",
    "    * 보통 단어 임베딩은 256, 512, 1024차원의 단어 임베딩을 사용.\n",
    "* B. 원-핫 인코딩이나 해싱으로 얻은 단어 표현은 희소하고 고차원이며, 수동으로 인코딩된다.\n",
    "* C. 단어 임베딩은 조밀하고 비교적 저차원이며 데이터로부터 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c8b37f4",
   "metadata": {},
   "source": [
    "### 단어 임베딩 만드는 두가지 방법\n",
    "* 신경망을 학습하는 방법처럼 단어 벡터를 학습하기\n",
    "* 사전에 훈련된 단어 임베딩을 로드하기(pretrained word embedding)\n",
    "    * 구글의 토마스 미코로프 [word2vec 알고리즘](https://code.google.com/archive/p/word2vec)\n",
    "    * 스탠포드 대학교 : [GloVe](https://nlp.stanford.edu/projects/glove)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "048c91da",
   "metadata": {},
   "source": [
    "### Embedding 층을 사용하여 단어 임베딩 학습하기\n",
    "* 단어와 밀집 벡터를 연관 짓는 가장 간단한 방법은 랜덤하게 벡터를 선택하는 것.\n",
    "* 이 방식의 문제점은 임베딩 공간이 구조적이지 않다는 것.\n",
    "    * 예를 들어, accurate와 exact단어는 대부분 문장에서 비슷한 의미로 사용. 단, 다른 임베딩을 갖는다.\n",
    "    * 심층 신경망이 이런 임의의 구조적이지 않은 임베딩 공간을 이해하기는 어렵다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f879f5c",
   "metadata": {},
   "source": [
    "### 해결 제안\n",
    "* 단어 벡터 사이에 좀 더 추상적이고 기하학적인 관계를 얻으려면 단어 사이에 있는 의미 관계를 반영해야함.\n",
    "    * 단어 임베딩은 언어를 기하학적 공간에 매핑하는 것.\n",
    "    * 예를 들어 잘 구축된 임베딩 공간에서는 동의어가 비슷한 단어 벡터로 임베딩된다.\n",
    "    * 일반적으로 두 단어 벡터 사이의 거리(L2 거리)는 이 단어 사이의 의미 거리와 관계되어 있다.\n",
    "        * 멀리 떨어진 위치에 임베딩된 단어 의미는 서로 다른 반면\n",
    "        * 비슷한 단어들은 가까이 임베딩된다.\n",
    "* 문제는 사람의 언어를 완전히 매핑시킬 수 있는 이상적인 단어 임베딩 공간을 만드는 것이다. 이런 공간이 있을까?\n",
    "* 아마도 가능하겠지만, 완벽하게는 어려울 수 있다.\n",
    "* 다만, 최근에는 많은 발전을 이루었다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e136fdb8",
   "metadata": {},
   "source": [
    "### 케라스 실습 - IMDB 데이터를 이용한 Embedding 실습\n",
    "* Embedding 층(특정 단어를 나타내는) 정수 인덱스를 밀집 벡터로 매핑하는 딕셔너리로 이해\n",
    "* 정수를 입력받아, 내부 딕셔너리에서 이 정수와 연관된 벡터를 찾아 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9519cf1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'keras.layers.embeddings.Embedding'> <keras.layers.embeddings.Embedding object at 0x7fa36254e1c0>\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Embedding\n",
    "\n",
    "# Embedding 층은 적어도 두 개의 매개변수를 사용.\n",
    "# 가능한 토큰의 개수(여기서는 1,000으로 단어 인덱스 최댓값 + 1입니다)와 \n",
    "# 임베딩 차원(여기서는 64)입니다\n",
    "embedding_layer = Embedding(1000, 64)\n",
    "\n",
    "print( type(embedding_layer), embedding_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be613641",
   "metadata": {},
   "source": [
    "### Embedding층의 입력\n",
    "* (samples, sequence_length)\n",
    "    * samples : 샘플수\n",
    "    * sequence_length : 시퀀스 길이 ( 단순히 길이)\n",
    "    * 정수 텐서를 입력으로 받음. 2D텐서\n",
    "* 여기서 sequence_length가 작은 길이의 시퀀스는 0으로 패딩되고, 긴 시퀀스는 잘리게 됩니다.\n",
    "\n",
    "### Embedding층의 출력\n",
    "* (samples, sqeuence_length, embedding_dimensionality)\n",
    "    * samples : 샘플수\n",
    "    * sequence_length : 시퀀스 길이\n",
    "    * embedding_dimensionality : 임베딩 차원\n",
    "    * 출력은 3D 텐서가 된다.\n",
    "\n",
    "### Embedding층의 객체\n",
    "* 가중치는 다른 층과 마찬가지로 랜덤하게 초기화\n",
    "* 신경망의 학습을 통해 점차 조정되어진다.\n",
    "    * 훈련이 끝나면 임베딩 공간은 특정 문제에 특화된 구조를 많이 갖는다.\n",
    "* IMDB 데이터 셋\n",
    "    * 총 5만개로 이루어진 긍정 부정의 리뷰\n",
    "    * 이 데이터셋은 훈련 데이터 25,000개와 테스트 데이터 25,000개로 나뉘어 있고 각각 50%는 부정, 50%는 긍정 리뷰로 구성\n",
    "    * 스탠포드 대학의 앤드류 마스(Andrew Maas)가 수집한 데이터 셋\n",
    "* train_data는 여러개의 단어로 이루어진 리뷰. 리뷰 단어는 각각 매칭된 word index 값으로 이루어짐.\n",
    "* train_labels는 1(긍정), 0(부정)이 됨."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e260344f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import imdb\n",
    "from keras import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b633a766",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 특성으로 사용할 단어의 수\n",
    "max_features = 10000\n",
    "\n",
    "# 정수 리스트로 데이터를 로드합니다.\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=max_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b723f8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((25000,), (25000,), (25000,), (25000,))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8862052e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 5952, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print(X_train[0])\n",
    "print(y_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0b2cd720",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(218, [1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  리뷰의 길이와 10개 단어(인덱스) 보기\n",
    "len(X_train[0]), X_train[0][0:10]   # 단어가 218개 단어로 구성"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b4114a",
   "metadata": {},
   "source": [
    "### 다양한 길이가 리뷰가 있을 것이다.\n",
    "* 리뷰에서 맨 마지막 50개 단어를 얻고, 나머지는 버린다. 또는 길이가 짧다면 0으로 채운다.\n",
    "* 시퀀스의 길이가 50개로 한다.\n",
    "* 리스트 형태의 리뷰를 2D 정수 텐서로 변환 : preprocessing.sequence.pad_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f55d208",
   "metadata": {},
   "source": [
    "### 텍스트 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b915f6c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "변경 전 :  (25000,) (25000,)\n",
      "변경 후 :  (25000, 50) (25000, 50)\n"
     ]
    }
   ],
   "source": [
    "# 리스트를 (samples, maxlen) 크기의 2D 정수 텐서로 변환합니다.\n",
    "maxlen = 50\n",
    "\n",
    "X_train_n = preprocessing.sequence.pad_sequences(X_train, maxlen=maxlen)\n",
    "X_test_n = preprocessing.sequence.pad_sequences(X_test, maxlen=maxlen)\n",
    "\n",
    "print(\"변경 전 : \" , X_train.shape, X_test.shape)\n",
    "print(\"변경 후 : \", X_train_n.shape, X_test_n.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a0242d",
   "metadata": {},
   "source": [
    "### 왜 1D 텐서를 2D텐서로 변경하는가?\n",
    "* Embedding() 층은 다음과 같이 입력(1D텐서)을 받는다.\n",
    "* (samples, sequence_length)\n",
    "* Embedding층의 출력\n",
    "    * (samples, sqeuence_length, embedding_dimensionality)\n",
    "    * (단어 인덱스, 시퀀스 길이, 임베딩 차원-출력)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5913c7a2",
   "metadata": {},
   "source": [
    "### 모델 구축"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "21cc78ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Flatten, Dense, Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b3511cfc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maxlen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3679b849",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, 50, 8)             80000     \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 400)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 401       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 80,401\n",
      "Trainable params: 80,401\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-12 10:52:03.527918: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "# 나중에 임베딩된 입력을 \n",
    "# Flatten 층에서 펼치기 위해 Embedding 층에 input_length를 지정\n",
    "# Embedding 층의 입력은 2D (samples, maxlen) 이다.\n",
    "model.add(Embedding(10000, 8, input_length=maxlen))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation = \"sigmoid\"))\n",
    "\n",
    "# Embedding 층의 출력 크기는 (samples, maxlen, 8)가 됩니다.\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9e1c0078",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "625/625 [==============================] - 1s 978us/step - loss: 0.6476 - acc: 0.6597 - val_loss: 0.5500 - val_acc: 0.7574\n",
      "Epoch 2/10\n",
      "625/625 [==============================] - 1s 855us/step - loss: 0.4536 - acc: 0.8032 - val_loss: 0.4323 - val_acc: 0.7958\n",
      "Epoch 3/10\n",
      "625/625 [==============================] - 1s 853us/step - loss: 0.3656 - acc: 0.8407 - val_loss: 0.4036 - val_acc: 0.8118\n",
      "Epoch 4/10\n",
      "625/625 [==============================] - 1s 872us/step - loss: 0.3240 - acc: 0.8604 - val_loss: 0.3978 - val_acc: 0.8164\n",
      "Epoch 5/10\n",
      "625/625 [==============================] - 0s 798us/step - loss: 0.2975 - acc: 0.8745 - val_loss: 0.3987 - val_acc: 0.8160\n",
      "Epoch 6/10\n",
      "625/625 [==============================] - 1s 836us/step - loss: 0.2760 - acc: 0.8850 - val_loss: 0.4033 - val_acc: 0.8124\n",
      "Epoch 7/10\n",
      "625/625 [==============================] - 1s 806us/step - loss: 0.2559 - acc: 0.8946 - val_loss: 0.4075 - val_acc: 0.8130\n",
      "Epoch 8/10\n",
      "625/625 [==============================] - 0s 794us/step - loss: 0.2367 - acc: 0.9062 - val_loss: 0.4175 - val_acc: 0.8124\n",
      "Epoch 9/10\n",
      "625/625 [==============================] - 1s 845us/step - loss: 0.2185 - acc: 0.9143 - val_loss: 0.4262 - val_acc: 0.8110\n",
      "Epoch 10/10\n",
      "625/625 [==============================] - 1s 961us/step - loss: 0.2005 - acc: 0.9243 - val_loss: 0.4349 - val_acc: 0.8086\n",
      "CPU times: user 7.88 s, sys: 1e+03 ms, total: 8.88 s\n",
      "Wall time: 5.79 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "# 분류기를 추가합니다.\n",
    "model.compile(optimizer='rmsprop', \n",
    "              loss='binary_crossentropy', \n",
    "              metrics=['acc'])\n",
    "\n",
    "history = model.fit(X_train_n, y_train,\n",
    "                    epochs=10,\n",
    "                    batch_size=32,\n",
    "                    validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5868bd13",
   "metadata": {},
   "source": [
    "* 검증 정확도가 80.3%입니다.\n",
    "* 리뷰 50개의 단어(시퀀스 길이)만 사용하여 좋은 결과를 얻었습니다.\n",
    "* 임베딩 층을 펼쳐 하나의 Dense 층을 이용하여 학습수행. 입력 시퀀스 있는 각 단어를 독립적으로 다루었습니다.\n",
    "* 단어 사이의 관계나 문장 구조를 고려하지 않음.\n",
    "    * 해결책 : 각 시퀀스 전체를 고려한 특성이 학습되도록 임베딩 층 위에 순환 층이나 1D 합성곱 층을 추가하는 것이 좋다.\n",
    "\n",
    "### 정리\n",
    "* 토큰(단어 등)를 벡터로 변환하는 방법\n",
    "    * 원핫 인코딩\n",
    "    * 원-핫 해싱 : : 각 단어에 명시적으로 인덱스를 할당. 임의의 사이즈에 데이터를 매핑.\n",
    "    * 단어 임베딩 사용\n",
    "        * 케라스에서 Embedding을 이용하여 일정단어를 일정 차원의 수로 단어를 벡터화 시킨다.\n",
    "        * 여기서의 가중치는 학습을 통해 특정 데이터에 특정된 임베딩 공간이 만들어진다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b08969",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
